{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-ap-northeast-2-137645371426\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket1 = sagemaker_session.default_bucket()\n",
    "print(bucket1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from HRC_0819_predict2 import get_matching_s3_keys\n",
    "bucket = 'hrms-aggr-train'\n",
    "prefix = 'regular'\n",
    "suffix = 'json'\n",
    "y = datetime.now().strftime(f'yyyy=%Y')\n",
    "m = datetime.now().strftime(f'mm=%m')\n",
    "d = datetime.now().strftime(f'dd=%d')\n",
    "data_dir = datetime.now().strftime('%Y-%m-%d')\n",
    "#######Fix Data Location#############\n",
    "d = 'dd=14'\n",
    "######################\n",
    "#print(data_dir)\n",
    "s3_nowdir=f'{prefix}/{y}/{m}/{d}'\n",
    "#print(s3_nowdir)\n",
    "#keys = [k for k in get_matching_s3_keys(bucket, s3_nowdir, suffix)]\n",
    "#print(keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://hrms-aggr-train/regular/yyyy=2020/mm=08/dd=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=f'data/', bucket=bucket, key_prefix=s3_nowdir)\n",
    "print(inputs)\n",
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/AmazonSageMaker-sagemaker-seoul/hrms\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/usr/bin/env python\u001b[39;49;00m\n",
      "\u001b[37m# coding: utf-8\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# In[2]:\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m metrics\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StandardScaler\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m \n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m mean_squared_error \u001b[34mas\u001b[39;49;00m mse\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msix\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BytesIO\n",
      "\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "        \u001b[36mself\u001b[39;49;00m.encoder = nn.Sequential(\n",
      "            nn.Linear(\u001b[34m6\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m),\n",
      "            nn.BatchNorm1d(\u001b[34m5\u001b[39;49;00m),\n",
      "            nn.LeakyReLU(),\n",
      "            nn.Linear(\u001b[34m5\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m),\n",
      "            nn.BatchNorm1d(\u001b[34m4\u001b[39;49;00m),\n",
      "            nn.LeakyReLU(),\n",
      "            nn.Linear(\u001b[34m4\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m),\n",
      "        )\n",
      "        \u001b[36mself\u001b[39;49;00m.decoder = nn.Sequential(\n",
      "            nn.Linear(\u001b[34m2\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m),\n",
      "            nn.BatchNorm1d(\u001b[34m4\u001b[39;49;00m),\n",
      "            nn.LeakyReLU(),\n",
      "            nn.Linear(\u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m),\n",
      "            nn.BatchNorm1d(\u001b[34m5\u001b[39;49;00m),\n",
      "            nn.LeakyReLU(),\n",
      "            nn.Linear(\u001b[34m5\u001b[39;49;00m, \u001b[34m6\u001b[39;49;00m),\n",
      "        )\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\n",
      "        z = \u001b[36mself\u001b[39;49;00m.encoder(x)\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.decoder(z)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_matching_s3_keys\u001b[39;49;00m(bucket, prefix=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, suffix=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Generate the keys in an S3 bucket.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    :param bucket: Name of the S3 bucket.\u001b[39;49;00m\n",
      "\u001b[33m    :param prefix: Only fetch keys that start with this prefix (optional).\u001b[39;49;00m\n",
      "\u001b[33m    :param suffix: Only fetch keys that end with this suffix (optional).\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    s3 = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    kwargs = {\u001b[33m'\u001b[39;49;00m\u001b[33mBucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: bucket}\n",
      "\n",
      "    \u001b[37m# If the prefix is a single string (not a tuple of strings), we can\u001b[39;49;00m\n",
      "    \u001b[37m# do the filtering directly in the S3 API.\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(prefix, \u001b[36mstr\u001b[39;49;00m):\n",
      "        kwargs[\u001b[33m'\u001b[39;49;00m\u001b[33mPrefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = prefix\n",
      "\n",
      "    \u001b[34mwhile\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m:\n",
      "\n",
      "        \u001b[37m# The S3 API response is a large blob of metadata.\u001b[39;49;00m\n",
      "        \u001b[37m# 'Contents' contains information about the listed objects.\u001b[39;49;00m\n",
      "        resp = s3.list_objects_v2(**kwargs)\n",
      "        \u001b[34mfor\u001b[39;49;00m obj \u001b[35min\u001b[39;49;00m resp[\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]:\n",
      "            key = obj[\u001b[33m'\u001b[39;49;00m\u001b[33mKey\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "            \u001b[34mif\u001b[39;49;00m key.startswith(prefix) \u001b[35mand\u001b[39;49;00m key.endswith(suffix):\n",
      "                \u001b[34myield\u001b[39;49;00m key\n",
      "\n",
      "        \u001b[37m# The S3 API is paginated, returning up to 1000 keys at a time.\u001b[39;49;00m\n",
      "        \u001b[37m# Pass the continuation token into the next response, until we\u001b[39;49;00m\n",
      "        \u001b[37m# reach the final page (when this field is missing).\u001b[39;49;00m\n",
      "        \u001b[34mtry\u001b[39;49;00m:\n",
      "            kwargs[\u001b[33m'\u001b[39;49;00m\u001b[33mContinuationToken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = resp[\u001b[33m'\u001b[39;49;00m\u001b[33mNextContinuationToken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[34mexcept\u001b[39;49;00m \u001b[36mKeyError\u001b[39;49;00m:\n",
      "            \u001b[34mbreak\u001b[39;49;00m    \n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mread_data\u001b[39;49;00m():\n",
      "    s3 = boto3.resource(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    response = s3.Object(\u001b[33m'\u001b[39;49;00m\u001b[33mhrms-train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtraindata/train.jsonl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).get()\n",
      "    data=[]\n",
      "    timestamp = []\n",
      "    robot_name = []\n",
      "    \u001b[34mfor\u001b[39;49;00m line \u001b[35min\u001b[39;49;00m response[\u001b[33m'\u001b[39;49;00m\u001b[33mBody\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]._raw_stream:\n",
      "        \u001b[34mtry\u001b[39;49;00m:\n",
      "            data.append(json.loads(line)[\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "            timestamp.append(json.loads(line)[\u001b[33m'\u001b[39;49;00m\u001b[33mtimestamp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "            robot_name.append(json.loads(line)[\u001b[33m'\u001b[39;49;00m\u001b[33mrobot_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "        \u001b[34mexcept\u001b[39;49;00m:\n",
      "            \u001b[34mpass\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m pd.DataFrame(data), timestamp, pd.DataFrame(robot_name, columns=[\u001b[33m'\u001b[39;49;00m\u001b[33mrobot_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(scaler, batch_size, training_dir, is_distributed, **kwargs):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    data, timestamp, robot_name = read_data()\n",
      "    data = scaler.fit_transform(data).astype(np.float32)\n",
      "    train_sampler = torch.utils.data.distributed.DistributedSampler(data) \u001b[34mif\u001b[39;49;00m is_distributed \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=train_sampler \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\n",
      "                                       sampler=train_sampler, **kwargs)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_test_data_loader\u001b[39;49;00m(scaler, test_batch_size, test_dir, **kwargs):\n",
      "    \u001b[37m# test data dir 명시 필요.\u001b[39;49;00m\n",
      "    data, timestamp, robot_name = read_data()\n",
      "    data = scaler.transform(data).astype(np.float32)\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet test data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\n",
      "        data, batch_size=test_batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m, **kwargs\n",
      "    ), timestamp, robot_name\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_average_gradients\u001b[39;49;00m(model):\n",
      "    \u001b[37m# Gradient averaging.\u001b[39;49;00m\n",
      "    size = \u001b[36mfloat\u001b[39;49;00m(dist.get_world_size())\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\n",
      "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
      "        param.grad.data /= size\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\n",
      "    scaler = StandardScaler()\n",
      "    is_distributed = \u001b[36mlen\u001b[39;49;00m(args.hosts) > \u001b[34m1\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.backend \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDistributed training - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(is_distributed))\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.num_gpus))\n",
      "    kwargs = {\u001b[33m'\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    label = args.label\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed:\n",
      "        \u001b[37m# Initialize the distributed environment.\u001b[39;49;00m\n",
      "        world_size = \u001b[36mlen\u001b[39;49;00m(args.hosts)\n",
      "        os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(world_size)\n",
      "        host_rank = args.hosts.index(args.current_host)\n",
      "        os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mRANK\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(host_rank)\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInitialized the distributed environment: \u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33m backend on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m nodes. \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "            args.backend, dist.get_world_size()) + \u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host rank is \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m. Number of gpus: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "            dist.get_rank(), args.num_gpus))\n",
      "\n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\n",
      "    torch.manual_seed(args.seed)\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\n",
      "        torch.cuda.manual_seed(args.seed)\n",
      "\n",
      "    train_loader = _get_train_data_loader(scaler, args.batch_size, args.data_dir, is_distributed, **kwargs)\n",
      "    test_loader, timestamp, robot_name = _get_test_data_loader(scaler, args.test_batch_size, args.data_dir, **kwargs)\n",
      "\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler), \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
      "        \u001b[34m100.\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(train_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\n",
      "    ))\n",
      "\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of test data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "        \u001b[36mlen\u001b[39;49;00m(test_loader.sampler), \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "        \u001b[34m100.\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(test_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "    ))\n",
      "\n",
      "    model = Net().to(device)\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m use_cuda:\n",
      "        \u001b[37m# multi-machine multi-gpu case\u001b[39;49;00m\n",
      "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[37m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001b[39;49;00m\n",
      "        model = torch.nn.DataParallel(model)\n",
      "\n",
      "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        model.train()\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, data \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\n",
      "            data = data.to(device)\n",
      "            optimizer.zero_grad()\n",
      "            output = model(data)\n",
      "            loss = F.mse_loss(data , output)\n",
      "            loss.backward()\n",
      "            \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m use_cuda:\n",
      "                \u001b[37m# average gradients manually for multi-machine cpu case only\u001b[39;49;00m\n",
      "                _average_gradients(model)\n",
      "            optimizer.step()\n",
      "            \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\n",
      "                logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "                    epoch, batch_idx * \u001b[36mlen\u001b[39;49;00m(data), \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\n",
      "                    \u001b[34m100.\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader), loss.item()))\n",
      "        test(model, test_loader, device, label, timestamp, robot_name, args.model_dir)\n",
      "    save_model(model, args.model_dir)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, device, label, timestamp, robot_name, model_dir):\n",
      "    \u001b[37m# anomaly score infrence\u001b[39;49;00m\n",
      "    model.eval()\n",
      "    y_pred = []\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m data \u001b[35min\u001b[39;49;00m test_loader:\n",
      "            data = data.to(device)\n",
      "            output = model(data)\n",
      "\u001b[37m#             print(output)\u001b[39;49;00m\n",
      "            y_pred += [F.mse_loss(data, output, reduction=\u001b[33m\"\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).mean(\u001b[34m1\u001b[39;49;00m)]\n",
      "    y_pred = torch.cat(y_pred).cpu().numpy()\n",
      "\u001b[37m#    output = pd.DataFrame(y_pred.astype(str), columns=['Anomaly score'])\u001b[39;49;00m\n",
      "\u001b[37m#    output_example = pd.concat([robot_name, output], axis=1)\u001b[39;49;00m\n",
      "\u001b[37m#    output_example.index = timestamp\u001b[39;49;00m\n",
      "\u001b[37m#    output_example.index.name = 'timestamp'\u001b[39;49;00m\n",
      "\u001b[37m#    print(output_example)\u001b[39;49;00m\n",
      "\u001b[37m#    filename = \"output_example.csv\"\u001b[39;49;00m\n",
      "\u001b[37m#    output_example.to_csv(filename)\u001b[39;49;00m\n",
      "\u001b[37m#    s3 = boto3.resource('s3')\u001b[39;49;00m\n",
      "\u001b[37m#    s3.meta.client.upload_file(filename, 'hrms-model', 'output_example.csv')\u001b[39;49;00m\n",
      "\u001b[37m#     s3 = boto3.client('s3')\u001b[39;49;00m\n",
      "\u001b[37m#     s3.upload_file('output_example', 'hrms-inference', 'outout')\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mAnomaly score \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.join(y_pred.astype(\u001b[36mstr\u001b[39;49;00m))\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = torch.nn.DataParallel(Net())\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n",
      "\n",
      "\u001b[37m# def input_fn(input_path, request_content_type):\u001b[39;49;00m\n",
      "\u001b[37m#     \"\"\"An input_fn that loads a pickled tensor\"\"\"\u001b[39;49;00m\n",
      "\u001b[37m#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\u001b[39;49;00m\n",
      "\u001b[37m#     scaler = StandardScaler()\u001b[39;49;00m\n",
      "\u001b[37m#     data, timestamp, robot_name = read_data()\u001b[39;49;00m\n",
      "\u001b[37m#     request_body = scaler.fit_transform(data).astype(np.float32)\u001b[39;49;00m\n",
      "\u001b[37m#     if request_content_type == 'application/python-pickle':\u001b[39;49;00m\n",
      "\u001b[37m#         return torch.load(BytesIO(request_body))\u001b[39;49;00m\n",
      "\u001b[37m#     else:\u001b[39;49;00m\n",
      "\u001b[37m#         # Handle other content-types here or raise an Exception\u001b[39;49;00m\n",
      "\u001b[37m#         # if the content type is not supported.\u001b[39;49;00m\n",
      "\u001b[37m#         pass\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# def input_fn(input_path, content_type='application/json'):\u001b[39;49;00m\n",
      "\u001b[37m#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\u001b[39;49;00m\n",
      "\u001b[37m#     scaler = StandardScaler()\u001b[39;49;00m\n",
      "\u001b[37m#     data = read_data(input_path)\u001b[39;49;00m\n",
      "\u001b[37m#     data = scaler.fit_transform(data).astype(np.float32)\u001b[39;49;00m\n",
      "\u001b[37m#     return data.to(device)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# def input_fn(request_body, content_type='application/json'):\u001b[39;49;00m\n",
      "\u001b[37m#     logger.info('Deserializing the input data.')\u001b[39;49;00m\n",
      "\u001b[37m#     if content_type == 'application/json':\u001b[39;49;00m\n",
      "\u001b[37m#         input_data = json.loads(request_body)\u001b[39;49;00m\n",
      "\u001b[37m#         url = input_data['url']\u001b[39;49;00m\n",
      "\u001b[37m#         logger.info(f'Image url: {url}')\u001b[39;49;00m\n",
      "\u001b[37m#         image_data = Image.open(requests.get(url, stream=True).raw)\u001b[39;49;00m\n",
      "        \n",
      "\u001b[37m#         image_transform = transforms.Compose([\u001b[39;49;00m\n",
      "\u001b[37m#             transforms.Resize(size=256),\u001b[39;49;00m\n",
      "\u001b[37m#             transforms.CenterCrop(size=224),\u001b[39;49;00m\n",
      "\u001b[37m#             transforms.ToTensor(),\u001b[39;49;00m\n",
      "\u001b[37m#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\u001b[39;49;00m\n",
      "\u001b[37m#         ])\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#         return image_transform(image_data)\u001b[39;49;00m\n",
      "\u001b[37m#     raise Exception(f'Requested unsupported ContentType in content_type {content_type}')\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# def predict_fn(input_data, model):\u001b[39;49;00m\n",
      "\u001b[37m#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\u001b[39;49;00m\n",
      "\u001b[37m#     model.to(device)\u001b[39;49;00m\n",
      "\u001b[37m#     model.eval()\u001b[39;49;00m\n",
      "\u001b[37m#     scaler = StandardScaler()\u001b[39;49;00m\n",
      "\u001b[37m#     with torch.no_grad():\u001b[39;49;00m\n",
      "\u001b[37m#         y_pred = []\u001b[39;49;00m\n",
      "\u001b[37m#         output = model(input_data)\u001b[39;49;00m\n",
      "\u001b[37m#         for i in range(len(data)):\u001b[39;49;00m\n",
      "\u001b[37m#             loss = mse(data[i],output[i])\u001b[39;49;00m\n",
      "\u001b[37m#             y_pred.append(f'{loss}')\u001b[39;49;00m\n",
      "\u001b[37m#     output1 = pd.DataFrame(y_pred, columns=['Anomaly score'])\u001b[39;49;00m\n",
      "\u001b[37m#     #data1, stats, timestamp, robot_name = read_data(input_path)\u001b[39;49;00m\n",
      "\u001b[37m#     #output_example = pd.concat([robot_name, output1], axis=1)\u001b[39;49;00m\n",
      "\u001b[37m#     #output_example.index = timestamp\u001b[39;49;00m\n",
      "\u001b[37m#     #output_example.index.name = 'timestamp'\u001b[39;49;00m\n",
      "\u001b[37m#     return output1\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# def output_fn(prediction, content_type):\u001b[39;49;00m\n",
      "\u001b[37m#     y_pred = []\u001b[39;49;00m\n",
      "\u001b[37m#     for i in range(len(prediction)):\u001b[39;49;00m\n",
      "\u001b[37m#         loss = mse(data[i],output[i])\u001b[39;49;00m\n",
      "\u001b[37m#         y_pred.append(f'{loss}')\u001b[39;49;00m\n",
      "\u001b[37m#     output1 = pd.DataFrame(y_pred, columns=['Anomaly score'])\u001b[39;49;00m\n",
      "\u001b[37m#     #data1, stats, timestamp, robot_name = read_data(input_path)\u001b[39;49;00m\n",
      "\u001b[37m#     #output_example = pd.concat([robot_name, output1], axis=1)\u001b[39;49;00m\n",
      "\u001b[37m#     #output_example.index = timestamp\u001b[39;49;00m\n",
      "\u001b[37m#     #output_example.index.name = 'timestamp'\u001b[39;49;00m\n",
      "\u001b[37m#     return output1\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_npy_dumps\u001b[39;49;00m(data):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Serialized a numpy array into a stream of npy-formatted bytes.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    buffer = BytesIO()\n",
      "    np.save(buffer, data)\n",
      "    \u001b[34mreturn\u001b[39;49;00m buffer.getvalue()\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept=\u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"This function is called on the return value of predict_fn, and is used to serialize the\u001b[39;49;00m\n",
      "\u001b[33m    predictions back to the client.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    This implementation is effectively identical to the default implementation used in the Chainer\u001b[39;49;00m\n",
      "\u001b[33m    container, for NPY formatted data. This function is included in this script to demonstrate\u001b[39;49;00m\n",
      "\u001b[33m    how one might implement `output_fn`.\u001b[39;49;00m\n",
      "\u001b[33m    Args:\u001b[39;49;00m\n",
      "\u001b[33m        prediction_output (numpy array): a numpy array containing the data serialized by the Chainer predictor\u001b[39;49;00m\n",
      "\u001b[33m        accept: the MIME type of the data expected by the client.\u001b[39;49;00m\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\n",
      "\u001b[33m        a tuple containing a serialized NumPy array and the MIME type of the serialized data.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m accept == \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "\u001b[37m#         output = _npy_dumps(prediction_output)\u001b[39;49;00m\n",
      "        output = prediction_output\n",
      "        y_pred = []\n",
      "        data, timestamp, robot_name  = read_data()\n",
      "        scaler = StandardScaler()\n",
      "        data= scaler.fit_transform(data).astype(np.float32)\n",
      "        \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(output)):\n",
      "            loss = mse(data[i],output[i])\n",
      "            y_pred.append(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mloss\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        output1 = pd.DataFrame(y_pred, columns=[\u001b[33m'\u001b[39;49;00m\u001b[33mAnomaly score\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "        output_example = pd.concat([robot_name, output1], axis=\u001b[34m1\u001b[39;49;00m)\n",
      "        output_example.index = timestamp\n",
      "        output_example.index.name = \u001b[33m'\u001b[39;49;00m\u001b[33mtimestamp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m        \n",
      "        output2 = output1.as_matrix().astype(np.float32)\n",
      "        \u001b[37m#Serialise numpy ndarray as bytes\u001b[39;49;00m\n",
      "        buffer = BytesIO()\n",
      "        np.save(buffer, output2)\n",
      "    \n",
      "\u001b[37m#         output1 = _npy_dumps(output1)\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m buffer.getvalue(), \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    \u001b[34melif\u001b[39;49;00m accept == \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33moutput_fn input is\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, prediction_output, \u001b[33m'\u001b[39;49;00m\u001b[33min format\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, accept)\n",
      "        \u001b[34mreturn\u001b[39;49;00m worker.Response(encoders.encode(prediction_output, accept), accept, mimetype=accept)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mAccept header must be application/x-npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[37m# def output_fn(prediction_output, accept='application/x-npy'):\u001b[39;49;00m\n",
      "\u001b[37m#     \"\"\"This function is called on the return value of predict_fn, and is used to serialize the\u001b[39;49;00m\n",
      "\u001b[37m#     predictions back to the client.\u001b[39;49;00m\n",
      "    \n",
      "\u001b[37m#     This implementation is effectively identical to the default implementation used in the Chainer\u001b[39;49;00m\n",
      "\u001b[37m#     container, for NPY formatted data. This function is included in this script to demonstrate\u001b[39;49;00m\n",
      "\u001b[37m#     how one might implement `output_fn`.\u001b[39;49;00m\n",
      "\u001b[37m#     Args:\u001b[39;49;00m\n",
      "\u001b[37m#         prediction_output (numpy array): a numpy array containing the data serialized by the Chainer predictor\u001b[39;49;00m\n",
      "\u001b[37m#         accept: the MIME type of the data expected by the client.\u001b[39;49;00m\n",
      "\u001b[37m#     Returns:\u001b[39;49;00m\n",
      "\u001b[37m#         a tuple containing a serialized NumPy array and the MIME type of the serialized data.\u001b[39;49;00m\n",
      "\u001b[37m#     \"\"\"\u001b[39;49;00m\n",
      "\u001b[37m#     if accept == 'application/x-npy':\u001b[39;49;00m\n",
      "\u001b[37m#         output = _npy_dumps(prediction_output)\u001b[39;49;00m\n",
      "\u001b[37m#         y_pred = []\u001b[39;49;00m\n",
      "\u001b[37m#         data, timestamp, robot_name  = read_data()\u001b[39;49;00m\n",
      "\u001b[37m#         scaler = StandardScaler()\u001b[39;49;00m\n",
      "\u001b[37m#         data= scaler.fit_transform(data).astype(np.float32)\u001b[39;49;00m\n",
      "\u001b[37m#         for i in range(len(output)):\u001b[39;49;00m\n",
      "\u001b[37m#             loss = mse(data[i],output[i])\u001b[39;49;00m\n",
      "\u001b[37m#             y_pred.append(f'{loss}')\u001b[39;49;00m\n",
      "\u001b[37m#         output1 = pd.DataFrame(y_pred, columns=['Anomaly score'])\u001b[39;49;00m\n",
      "\u001b[37m#         output_example = pd.concat([robot_name, output1], axis=1)\u001b[39;49;00m\n",
      "\u001b[37m#         output_example.index = timestamp\u001b[39;49;00m\n",
      "\u001b[37m#         output_example.index.name = 'timestamp'        \u001b[39;49;00m\n",
      "\u001b[37m#         return output_example.to_numpy(), 'application/x-npy'\u001b[39;49;00m\n",
      "\u001b[37m#     elif accept == 'application/json':\u001b[39;49;00m\n",
      "\u001b[37m#         print('output_fn input is', prediction_output, 'in format', accept)\u001b[39;49;00m\n",
      "\u001b[37m#         return worker.Response(encoders.encode(prediction_output, accept), accept, mimetype=accept)\u001b[39;49;00m\n",
      "\u001b[37m#     else:\u001b[39;49;00m\n",
      "\u001b[37m#         raise ValueError('Accept header must be application/x-npy')\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[37m#suffix = str(int(pd.Timestamp.now().timestamp()))\u001b[39;49;00m\n",
      "    \u001b[37m#filename = f'model.pth'\u001b[39;49;00m\n",
      "    path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[37m# recommended way from http://pytorch.org/docs/master/notes/serialization.html\u001b[39;49;00m\n",
      "    torch.save(model.cpu().state_dict(), path)\n",
      "    \u001b[37m#s3 = boto3.client('s3')\u001b[39;49;00m\n",
      "    \u001b[37m#s3.upload_file(path, 'hrms-model', f'model.pth')\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1000\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for testing (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m)\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    train(parser.parse_args())\n"
     ]
    }
   ],
   "source": [
    "import pygments\n",
    "!pygmentize ./HRC_0819_predict2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-26 04:23:58 Starting - Starting the training job...\n",
      "2020-08-26 04:24:01 Starting - Launching requested ML instances......\n",
      "2020-08-26 04:25:09 Starting - Preparing the instances for training...\n",
      "2020-08-26 04:25:49 Downloading - Downloading input data...\n",
      "2020-08-26 04:26:28 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:41,803 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:41,806 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:41,814 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:43,229 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:43,480 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:43,480 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:43,480 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:43,480 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmp1yv2zfoi/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=12977 sha256=11753a48d4e6556bb9f35b27e5b6dae20d4ccf80f069005673cbc22d6b0d1aef\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zyk32v51/wheels/78/1f/7c/f42560e6cd37e88752da1561426c2e4f9ae585096f45979d34\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "2020-08-26 04:26:59 Uploading - Uploading generated training model\n",
      "2020-08-26 04:26:59 Completed - Training job completed\n",
      "\u001b[34m2020-08-26 04:26:45,540 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:45,551 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:45,561 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:45,570 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 64,\n",
      "        \"test-batch-size\": 64,\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-08-26-04-23-57-976\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://hrms-model/pytorch-training-2020-08-26-04-23-57-976/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"HRC_0819_predict2\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"HRC_0819_predict2.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":6,\"test-batch-size\":64}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=HRC_0819_predict2.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=HRC_0819_predict2\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://hrms-model/pytorch-training-2020-08-26-04-23-57-976/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":6,\"test-batch-size\":64},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-08-26-04-23-57-976\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://hrms-model/pytorch-training-2020-08-26-04-23-57-976/source/sourcedir.tar.gz\",\"module_name\":\"HRC_0819_predict2\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"HRC_0819_predict2.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"64\",\"--epochs\",\"6\",\"--test-batch-size\",\"64\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_TEST-BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python HRC_0819_predict2.py --backend gloo --batch-size 64 --epochs 6 --test-batch-size 64\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 47/47 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 47/47 (100%) of test data\u001b[0m\n",
      "\u001b[34m[2020-08-26 04:26:48.693 algo-1:55 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-26 04:26:48.693 algo-1:55 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-26 04:26:48.693 algo-1:55 INFO hook.py:236] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-26 04:26:48.693 algo-1:55 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-08-26 04:26:48.694 algo-1:55 INFO hook.py:376] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-08-26 04:26:48.694 algo-1:55 INFO hook.py:437] Hook is writing from the hook with pid: 55\n",
      "\u001b[0m\n",
      "\u001b[34mAnomaly score 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 7.2870307, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016, 0.070050016\u001b[0m\n",
      "\u001b[34mAnomaly score 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 7.2794747, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987, 0.06875987\u001b[0m\n",
      "\u001b[34mAnomaly score 0.06743694, 7.272035, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694, 0.06743694\u001b[0m\n",
      "\u001b[34mAnomaly score 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 7.26519, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642, 0.06618642\u001b[0m\n",
      "\u001b[34mAnomaly score 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 7.259899, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166, 0.065057166\u001b[0m\n",
      "\u001b[34mAnomaly score 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 7.2564416, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235, 0.064041235\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34m[2020-08-26 04:26:48.812 algo-1:55 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-08-26 04:26:49,056 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 70\n",
      "Billable seconds: 70\n"
     ]
    }
   ],
   "source": [
    "# ipython nbconvert --to python *.ipynb\n",
    "import pandas as pd\n",
    "s3_output_location = 's3://{}'.format('hrms-model')\n",
    "estimator = PyTorch(entry_point='HRC_0819_predict2.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.c5.4xlarge',\n",
    "                    output_path=s3_output_location,\n",
    "                    hyperparameters={\n",
    "                        'epochs': 6,\n",
    "                        'backend': 'gloo',\n",
    "                        'batch-size': 64,\n",
    "                        'test-batch-size': 64,\n",
    "                    })\n",
    "\n",
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.c5.4xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-8y61r_1  |\u001b[0m 2020-08-24 01:36:35,173 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1\n",
      "\u001b[36malgo-1-8y61r_1  |\u001b[0m 2020-08-24 01:36:35,174 [INFO ] W-9000-model ACCESS_LOG - /172.18.0.1:49650 \"POST /invocations HTTP/1.1\" 500 5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-a8f75d9a6096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TEST##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/2020-08-18/train.jsonl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m_handle_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserializer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;31m# It's the deserializer's responsibility to close the stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ContentType\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mresponse_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, stream, content_type)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCONTENT_TYPE_NPY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                 raise ValueError(\"Cannot load file containing pickled data \"\n\u001b[0m\u001b[1;32m    458\u001b[0m                                  \"when allow_pickle=False\")\n\u001b[1;32m    459\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "#TEST##\n",
    "output = predictor.predict('data/2020-08-18/train.jsonl')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HRC_0819_predict2 import read_data\n",
    "data, timestamp, robot_name  = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.         -6.78233     0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]\n",
      " [ 0.          0.          0.          0.          0.14744195  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "scaler = StandardScaler()\n",
    "data= scaler.fit_transform(data).astype(np.float32)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"'bytes' object has no attribute 'read'\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 125, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 216, in _default_transform_fn\n    result = self._output_fn(prediction, accept)\n  File \"/opt/ml/model/code/HRC_0819_predict2.py\", line 351, in output_fn\n    return buffer.getvalue().read().decode(), 'application/x-npy'\nAttributeError: 'bytes' object has no attribute 'read'\n\". See https://ap-northeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-northeast-2#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-training-2020-08-26-02-49-39-489 in account 137645371426 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7e9cc148a40f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"'bytes' object has no attribute 'read'\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 125, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 216, in _default_transform_fn\n    result = self._output_fn(prediction, accept)\n  File \"/opt/ml/model/code/HRC_0819_predict2.py\", line 351, in output_fn\n    return buffer.getvalue().read().decode(), 'application/x-npy'\nAttributeError: 'bytes' object has no attribute 'read'\n\". See https://ap-northeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-northeast-2#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-training-2020-08-26-02-49-39-489 in account 137645371426 for more information."
     ]
    }
   ],
   "source": [
    "output = predictor.predict(data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.dtype\n",
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-0af1fa900cec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "data.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimator.delete_endpoint() will be deprecated in SageMaker Python SDK v2. Please use the delete_endpoint() function on your predictor instead.\n"
     ]
    }
   ],
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
